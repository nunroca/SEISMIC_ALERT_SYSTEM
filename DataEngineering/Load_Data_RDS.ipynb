{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIBRERIAS NECESARIAS:\n",
    "#Para utilizar API\n",
    "import requests\n",
    "#Para realizar la estructura tabular\n",
    "import pandas as pd\n",
    "#Para rellenar vacíos\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "#ETL:\n",
    "\n",
    "#para normalizar strings\n",
    "from unicodedata import normalize\n",
    "#para normalizar incluyendo la ñ\n",
    "import re \n",
    "#para normalizar fechas\n",
    "import datetime\n",
    "#para obtener fechas\n",
    "import time\n",
    "#hacer los calendarios de iteración\n",
    "from dateutil.rrule import rrule, DAILY , MONTHLY\n",
    "\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#para normalizar fechas\n",
    "from datetime import datetime\n",
    "#para obtener fechas\n",
    "import time\n",
    "\n",
    "\n",
    "#ml\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "dic={}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Conexión al  servidos RDS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"database-alertas-sismicas.crcnepco0igw.us-east-1.rds.amazonaws.com\"\n",
    "user = \"admin\"\n",
    "password = \"admin4168\"\n",
    "database = \"alertasSismicas\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "con=create_engine(f\"mysql+mysqlconnector://{user}:{password}@{host}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(\n",
    "    host=host,\n",
    "    user=user,\n",
    "    password=password,\n",
    "    database=database\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelo_ml.pickle', 'rb') as ml:\n",
    "    model = pickle.load(ml) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Creation of data tables.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countries = 'DROP TABLE IF EXISTS COUNTRIES CASCADE; CREATE TABLE COUNTRIES (idcountry INTEGER AUTO_INCREMENT PRIMARY KEY NOT NULL ,country text);'\n",
    "data_jap = 'DROP TABLE IF EXISTS JAPAN CASCADE; CREATE TABLE JAPAN (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'\n",
    "data_usa = 'DROP TABLE IF EXISTS USA CASCADE; CREATE TABLE USA (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'\n",
    "data_chile = 'DROP TABLE IF EXISTS CHILE CASCADE; CREATE TABLE CHILE (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'\n",
    "data_fact = 'DROP TABLE IF EXISTS FACTS CASCADE; CREATE TABLE FACTS (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'\n",
    "data_tsu = 'DROP TABLE IF EXISTS TSUNAMIS CASCADE; CREATE TABLE TSUNAMIS (id SERIAL PRIMARY KEY NOT NULL ,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mxwatheight float8, place text, time timestamp, mag float8, lng float8, lat float8, depth float8);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = 'DROP TABLE IF EXISTS SISMOTEST CASCADE; CREATE TABLE SISMOTEST (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alarm = 'DROP TABLE IF EXISTS ALARMS CASCADE;CREATE TABLE ALARMS (idsismo SERIAL PRIMARY KEY NOT NULL,idcountry INTEGER,foreign key (idcountry) references COUNTRIES(idcountry),mag float8, place text,time timestamp,tsunami text,lng float8, lat float8,depth float8, danger smallint);'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_countries)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_jap)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_chile)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_usa)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_fact)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_tsu)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_test)\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor=connection.cursor()\n",
    "cursor.execute(data_alarm)\n",
    "cursor.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Development of the Countries table. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def COUNTRIES(country):\n",
    "\n",
    "    global dic\n",
    "    if country in dic:\n",
    "      \n",
    "        return(dic[country])\n",
    "    else:\n",
    "        dic[country]=len(dic)+1\n",
    "        dic_pais={'COUNTRY':[country]}\n",
    "        df=pd.DataFrame(dic_pais)\n",
    "        #df.to_sql(name='COUNTRIES',con=con, if_exists='append', index=False)\n",
    "        \n",
    "        return(dic[country])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTRIES(\"USA\")\n",
    "COUNTRIES(\"CHILE\")\n",
    "COUNTRIES(\"JAPAN\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Development of the Historic table. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_japan = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2010-01-01%2000:00:00&endtime=2023-12-31%2023:59:59&minlatitude=27.000000&maxlatitude=44.000000&minlongitude=132.780000&maxlongitude=145.530000&minmagnitude=3&orderby=time-asc&jsonerror=true'\n",
    "url_chile = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2010-01-01%2000:00:00&endtime=2023-12-31%2023:59:59&minlatitude=-56.800000&maxlatitude=-19.000000&minlongitude=-79.000000&maxlongitude=-69.&minmagnitude=3&orderby=time-asc&jsonerror=true'\n",
    "url_usa = f'https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime=2010-01-01%2000:00:00&endtime=2023-12-31%2023:59:59&maxlatitude=50&minlatitude=24.6&maxlongitude=-65&minlongitude=-125&minmagnitude=3&orderby=time-asc&jsonerror=true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guardar_datos (url):\n",
    "    '''\n",
    "    Función: pasar de los datos de url de USGS\n",
    "    Entrada: Toma de parámetro una url y devuelve el df que utilizaremos\n",
    "    Salida: DataFrame con los datos de los sismos en la fecha\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Obtenemos los datos\n",
    "    resp = requests.get(url).json()\n",
    "\n",
    "    # Guardamos los datos en formato diccionario\n",
    "    dict={'mag':[],'place':[],'time':[],'url':[],'tsunami':[],'title':[],'lng':[],'lat':[],'deepth':[]}\n",
    "    #recorremos la catidad de \"filas\" que tiene\n",
    "    for i in range(len(resp['features'])):\n",
    "        \n",
    "        a=resp['features'][i]\n",
    "        #agrego al diccionario magnitud, tiempo, si produjo tsunami \n",
    "        lista_propierties=['mag','time']\n",
    "        for elemento in lista_propierties:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(np.nan) \n",
    "            else:\n",
    "                dict[elemento].append(float(a['properties'][elemento]))\n",
    "                \n",
    "        #agrego lugar, url con información a ampliar y el título     \n",
    "        lista_propierties2=['place','url','title','tsunami']\n",
    "        for elemento in lista_propierties2:\n",
    "            if a['properties'][elemento]is None:\n",
    "                dict[elemento].append(None)\n",
    "            else:\n",
    "                dict[elemento].append(a['properties'][elemento])\n",
    "\n",
    "        #Agrego al diccionario latitud, longitud y profundidad\n",
    "        list_geometry=['lng','lat','deepth']\n",
    "        for indice, elemento in enumerate(list_geometry):\n",
    "            if a['geometry']['coordinates'][indice] is None:\n",
    "                dict[elemento].append(np.nan)\n",
    "            else:\n",
    "                dict[elemento].append(float(a['geometry']['coordinates'][indice]))\n",
    "\n",
    "    #Devuelvo el diccionario hecho df\n",
    "    return (pd.DataFrame(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chil_prev=guardar_datos(url_chile)\n",
    "df_jap_prev=guardar_datos(url_japan)\n",
    "df_usa_prev=guardar_datos(url_usa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformar_fecha(dato):\n",
    "\n",
    "    #corregimos las fechas\n",
    "    dato=dato.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "\n",
    "    #pasamos a formato fecha\n",
    "    dato.time=dato.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "    return dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nor_danger(a):\n",
    "    b=0\n",
    "    if   a==8: b=3\n",
    "    elif a==6: b=2\n",
    "    elif a==2: b=2\n",
    "    elif a==0: b=2\n",
    "    elif a==7: b=2\n",
    "    elif a==3: b=1\n",
    "    elif a==9: b=1\n",
    "    elif a==1: b=1\n",
    "    elif a==4: b=1\n",
    "    return(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generat_dat (df,pais:str):\n",
    "    \n",
    "    import datetime\n",
    "    df[\"place\"]=df[\"place\"].fillna(\"no data\")\n",
    "    df=df.fillna(0)\n",
    "    df=df.drop_duplicates()\n",
    "    df=df[[\"mag\",\"place\",\"time\",\"lng\",\"lat\",\"deepth\",\"tsunami\"]]\n",
    "    \n",
    "    #corregimos las fechas\n",
    "    df.time=df.time.apply(lambda x: datetime.datetime.fromtimestamp(int(x)//1000).strftime('%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "\n",
    "    #pasamos a formato fecha\n",
    "    df.time=df.time.apply(lambda x: datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S.%f') if x!=np.nan else x)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    df.rename(columns={'deepth':'depth' }, inplace=True)\n",
    "    df[\"idcountry\"]=COUNTRIES(pais)\n",
    "    df[\"danger\"]=\"1\"\n",
    "    \n",
    "    #model\n",
    "    x = np.array(df[[\"idcountry\",\"mag\",\"tsunami\",\"depth\"]])\n",
    "    result_x = model.predict(x)\n",
    "    df[\"danger\"]=result_x\n",
    "    \n",
    "    #danger\n",
    "    df[\"danger\"]=df[\"danger\"].apply(lambda x: nor_danger(x))\n",
    "    \n",
    "    df=df[[\"idcountry\",\"mag\",\"place\",\"time\",\"tsunami\",\"lng\",\"lat\",\"depth\",\"danger\"]]\n",
    "    \n",
    "         \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_his_chil=generat_dat(df_chil_prev,\"CHILE\")\n",
    "dt_his_usa=generat_dat(df_usa_prev,\"USA\")\n",
    "dt_his_jap=generat_dat(df_jap_prev,\"JAPAN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14723"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_his_usa.to_sql(name='USA',con=con, if_exists='append', index=False)\n",
    "dt_his_usa.to_sql(name='FACTS',con=con, if_exists='append', index=False)\n",
    "\n",
    "dt_his_chil.to_sql(name='CHILE',con=con, if_exists='append', index=False)\n",
    "dt_his_chil.to_sql(name='FACTS',con=con, if_exists='append', index=False)\n",
    "\n",
    "dt_his_jap.to_sql(name='JAPAN',con=con, if_exists='append', index=False)\n",
    "dt_his_jap.to_sql(name='FACTS',con=con, if_exists='append', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Generate data from tsunamis. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?country=USA&maxYear=2023&minYear=2010'\n",
    "dt_usa_prev_Mar = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?country=JAPAN&maxYear=2023&minYear=2010'\n",
    "dt_jap_prev_Mar = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/tsunamis/events?country=CHILE&maxYear=2023&minYear=2010'\n",
    "dt_chi_prev_Mar = pd.DataFrame(requests.get(url).json()['items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mar(df):\n",
    "    \n",
    "    df=df.fillna(0)\n",
    "    df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']],format='%Y/%m/%d %H:%M:%S')\n",
    "    df['time'] =df['time'].apply(lambda x: x.strftime('%Y/%m/%d %H:%M:%S'))\n",
    "    df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'depth', 'latitude':'lat', 'longitude':'lng',\n",
    "    'locationName':'place', 'maxWaterHeight':'mxwatheight'}, inplace=True)\n",
    "\n",
    "\n",
    "    df['mxwatheight']  = df.mxwatheight.astype('float64')\n",
    "    df.mag=df.mag.astype('float64')\n",
    "    df.lng=df.lng.astype('float64')\n",
    "    df.lat=df.lat.astype('float64')\n",
    "    df.depth=df.depth.astype('float64')\n",
    "\n",
    "    a=df[\"country\"][1]\n",
    "    df[\"idcountry\"]=COUNTRIES(a)\n",
    "    \n",
    "    # Filtramos el dataframe con los datos relevantes.\n",
    "    df = df[['idcountry','mxwatheight','place', 'time', 'mag', 'lng','lat', 'depth' ]]\n",
    "    \n",
    "    return (df)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_usa_Mar=mar(dt_usa_prev_Mar)\n",
    "dt_chi_Mar=mar(dt_chi_prev_Mar)\n",
    "dt_jap_Mar=mar(dt_jap_prev_Mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_usa_Mar.to_sql(name='TSUNAMIS',con=con, if_exists='append', index=False)\n",
    "dt_chi_Mar.to_sql(name='TSUNAMIS',con=con, if_exists='append', index=False)\n",
    "dt_jap_Mar.to_sql(name='TSUNAMIS',con=con, if_exists='append', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 20px; color:cyan; font-weight: bold;\"> - Generate datatest from Sismos. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sismos_Noaa():\n",
    "\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    countries = ['USA', 'CHILE', 'JAPAN']\n",
    "    dt=pd.DataFrame(index=['idpais','mag','place', 'time','tsunami', 'lng','lat', 'deepth'])\n",
    "    for country in countries:\n",
    "        if country==\"USA\": z=1\n",
    "        elif country==\"CHILE\": z=2\n",
    "        elif country==\"JAPAN\": z=3\n",
    "        # Obtenemos información a partir de la API\n",
    "        url = f'https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/earthquakes?country={country}&maxYear=2023&minYear=2000'\n",
    "        df = pd.DataFrame(requests.get(url).json()['items'])\n",
    "\n",
    "        # Creamos la columna con la fecha del sismo\n",
    "        df['time'] = pd.to_datetime(df[['year','month','day','hour','minute','second']])\n",
    "\n",
    "        # Renombramos los features para normalizar con los otros datos recolectados\n",
    "        df.rename(columns={'eqMagnitude':'mag', 'eqDepth':'depth', 'latitude':'lat', 'longitude':'lng', 'locationName':'place', 'country':'pais'}, inplace=True)\n",
    "\n",
    "        # Creamos features adicionales para normalizar los datos.\n",
    "        df['url'] = url\n",
    "        df['title'] = None\n",
    "        df['tsunami'] = df.apply(lambda x: 1 if pd.isnull(x.tsunamiEventId) != True else 0, axis=1)\n",
    "        df['idcountry']  = z\n",
    "        df['place']  = df.apply(lambda x: x.place.lower(),axis=1)\n",
    "        \n",
    "        # Filtramos el dataframe con los datos relevantes.\n",
    "        df = df[['idcountry','mag','place', 'time','tsunami', 'lng','lat', 'depth']]\n",
    "        df['danger']=3\n",
    "        dt=pd.concat([dt,df],ignore_index=True)\n",
    "        dt= dt.dropna(how='all')\n",
    "        dt=dt.sort_values(\"time\", ascending=False)\n",
    "\n",
    "    return(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_sismo=sismos_Noaa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_sismo.to_sql(name='SISMOTEST',con=con, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
